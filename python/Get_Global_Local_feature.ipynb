{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: cd: can't cd to Project\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Get_global_local_feature\n",
    "import scipy.signal as ss\n",
    "import pac\n",
    "import itertools\n",
    "import pickle\n",
    "import mne\n",
    "import os\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "%matplotlib inline\n",
    "\n",
    "# from numba import jit, cuda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get global feature using convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = '_1_200'#'_1ch_nv'\n",
    "gamma = [ 1, 200]\n",
    "beta  = [ 1, 50]\n",
    "\n",
    "filt_wind = np.ones((3, 3))\n",
    "filt_wind /= filt_wind.sum()\n",
    "\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "base_path = config['BASE_PATH']\n",
    "ds_path = os.path.join(base_path, 'ds003490-download')\n",
    "nbchan = len(config['channels']) - 1\n",
    "nbtime = 601\n",
    "\n",
    "base_shape = (3, 25, 3)\n",
    "mvl_shape = base_shape + (nbchan, gamma[1] - gamma[0] + 1, beta[1] - beta[0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvl_2ds_mmap_path = os.path.join(ds_path, f'MVL_2ds_{suffix}.mmap')\n",
    "mvl_2ds = np.memmap(mvl_2ds_mmap_path, dtype=float, mode='r',\n",
    "                    shape=mvl_shape)\n",
    "\n",
    "mvl_time_mmap_path = os.path.join(ds_path, f'PAC_time_{suffix}.mmap')\n",
    "mvl_time = np.memmap(mvl_time_mmap_path, dtype=float, mode='r',\n",
    "                    shape=mvl_shape + (6,))\n",
    "\n",
    "erp_mmap_path = os.path.join(ds_path, f'ERPs_{suffix}.mmap')\n",
    "epochs = np.memmap(erp_mmap_path, dtype=float, mode='r',\n",
    "                    shape=base_shape + (2, nbchan, nbtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on Kiani's PAC\n",
    "### new pipline with normalization of channel\n",
    "load PAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('..\\data\\mvl_2ds_1_50.npz')\n",
    "PAC = a['mvl_2ds']*1e-12\n",
    "Test = Get_global_local_feature.conv_PAC_dist(\n",
    "    PAC[0, 0, 0, 0, 0:64, :], h=4, w=4, down_sample_factor_h=4, down_sample_factor_w=4)\n",
    "Test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAC_feature_sub_on_electrodes = np.zeros(shape=(3, 25, 3, 63, 16, 4))\n",
    "\n",
    "\n",
    "@jit\n",
    "def PAC_caluculation():\n",
    "    for i in np.arange(3):\n",
    "        for j in np.arange(3):\n",
    "            for ch in np.arange(63):\n",
    "                for sub in np.arange(25):\n",
    "                    PAC_feature_sub_on_electrodes[i, sub, j, ch, :, :] = Get_global_local_feature.conv_PAC_dist(\n",
    "                        PAC[i, sub, j, ch, 0:64, :], h=4, w=4, down_sample_factor_h=4, down_sample_factor_w=4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PAC_caluculation()\n",
    "    np.save(f'..\\data\\Feature_window_dimension_4_4.npy',\n",
    "            PAC_feature_sub_on_electrodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load feature and set parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAC_feature_sub_on_electrodes = np.load(\n",
    "    f'..\\data\\Feature_window_dimension_4_4.npy')\n",
    "groups = ['PD Med Off', 'PD Med On', 'CTL']\n",
    "event_types = ['Target', 'Standard', 'Novelty']\n",
    "PAC_feature_sub_on_electrodes.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get indexes of selected channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\data\\channels_name.pkl', 'rb') as f:\n",
    "    channels_name = pickle.load(f)\n",
    "selected_channels = ['Fz', 'Pz', 'Cz', 'FCz']\n",
    "# selected_channels = ['FCz']\n",
    "list_index = [channels_name.index(i) for i in selected_channels]\n",
    "list_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Local Features for time PAC series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_PAC_dist(PAC_dist, h=4, w=4, down_sample_factor_h=4, down_sample_factor_w=4, filter_mode='valid'):\n",
    "    \"\"\"PAC_dist input PAC values\\\n",
    "    h filter kernel size along axis 0\\\n",
    "    w filter kernel size along axis 1\\\n",
    "    down sample factor h along axis 0\\\n",
    "    down sample factor w along axis 1\\\n",
    "    filter mode define the mode of convolve2d of scipy.signal\"\"\"\n",
    "    # Filtering\n",
    "    width_filter = w\n",
    "    height_filter = h\n",
    "    kernel_filter = np.ones(\n",
    "        shape=[width_filter, height_filter])/(width_filter*height_filter)\n",
    "    PAC_feature = ss.convolve2d(PAC_dist, kernel_filter, mode=filter_mode)\n",
    "\n",
    "    # Sampling\n",
    "    height_index = np.linspace(\n",
    "        0, PAC_feature.shape[0]-1, PAC_dist.shape[0]//down_sample_factor_h).astype(np.int32)\n",
    "    width_index = np.linspace(\n",
    "        0, PAC_feature.shape[0]-1, PAC_dist.shape[1]//down_sample_factor_w).astype(np.int32)\n",
    "    h, w = np.meshgrid(height_index, width_index)\n",
    "    return PAC_feature[w, h]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C_CONTIGUOUS : True\n",
      "  F_CONTIGUOUS : False\n",
      "  OWNDATA : False\n",
      "  WRITEABLE : True\n",
      "  ALIGNED : True\n",
      "  WRITEBACKIFCOPY : False\n",
      "  UPDATEIFCOPY : False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.1898s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=12)]: Done  37 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0968s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=12)]: Done  54 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=12)]: Done  75 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.1991s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=12)]: Done 104 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=12)]: Done 164 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=12)]: Done 256 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=12)]: Done 392 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=12)]: Done 528 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=12)]: Done 680 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=12)]: Done 832 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=12)]: Done 1000 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=12)]: Done 1168 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=12)]: Done 1352 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=12)]: Done 1536 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=12)]: Done 1736 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=12)]: Done 1936 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=12)]: Done 2152 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=12)]: Done 2368 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=12)]: Batch computation too slow (2.0901s.) Setting batch_size=1.\n",
      "[Parallel(n_jobs=12)]: Done 2600 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=12)]: Done 2734 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.1927s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.1808s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=12)]: Done 2814 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=12)]: Done 2900 tasks      | elapsed:   22.5s\n",
      "[Parallel(n_jobs=12)]: Done 3032 tasks      | elapsed:   23.6s\n",
      "[Parallel(n_jobs=12)]: Done 3164 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=12)]: Done 3304 tasks      | elapsed:   26.6s\n",
      "[Parallel(n_jobs=12)]: Done 3444 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=12)]: Done 3592 tasks      | elapsed:   29.3s\n",
      "[Parallel(n_jobs=12)]: Done 3740 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=12)]: Done 3896 tasks      | elapsed:   32.0s\n",
      "[Parallel(n_jobs=12)]: Done 4052 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=12)]: Done 4216 tasks      | elapsed:   34.9s\n",
      "[Parallel(n_jobs=12)]: Done 4380 tasks      | elapsed:   36.3s\n",
      "[Parallel(n_jobs=12)]: Done 4552 tasks      | elapsed:   37.6s\n",
      "[Parallel(n_jobs=12)]: Done 4724 tasks      | elapsed:   39.2s\n",
      "[Parallel(n_jobs=12)]: Done 4904 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=12)]: Done 5084 tasks      | elapsed:   41.7s\n",
      "[Parallel(n_jobs=12)]: Done 5272 tasks      | elapsed:   43.7s\n",
      "[Parallel(n_jobs=12)]: Done 5460 tasks      | elapsed:   45.1s\n",
      "[Parallel(n_jobs=12)]: Done 5656 tasks      | elapsed:   46.8s\n",
      "[Parallel(n_jobs=12)]: Batch computation too slow (2.0219s.) Setting batch_size=1.\n",
      "[Parallel(n_jobs=12)]: Done 5822 tasks      | elapsed:   49.4s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.1879s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=12)]: Done 5897 tasks      | elapsed:   49.8s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0915s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=12)]: Done 6032 tasks      | elapsed:   50.9s\n",
      "[Parallel(n_jobs=12)]: Done 6244 tasks      | elapsed:   52.6s\n",
      "[Parallel(n_jobs=12)]: Done 6456 tasks      | elapsed:   54.1s\n",
      "[Parallel(n_jobs=12)]: Done 6676 tasks      | elapsed:   56.0s\n",
      "[Parallel(n_jobs=12)]: Done 6896 tasks      | elapsed:   57.8s\n",
      "[Parallel(n_jobs=12)]: Done 7124 tasks      | elapsed:   59.7s\n",
      "[Parallel(n_jobs=12)]: Done 7352 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=12)]: Done 7588 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=12)]: Done 7824 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=12)]: Done 8068 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=12)]: Done 8312 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=12)]: Done 8564 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=12)]: Done 8816 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=12)]: Done 9076 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=12)]: Done 9336 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=12)]: Done 9604 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=12)]: Done 9872 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=12)]: Done 10148 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=12)]: Done 10424 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=12)]: Done 10708 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=12)]: Done 10992 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=12)]: Done 11284 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=12)]: Done 11576 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=12)]: Done 11876 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=12)]: Batch computation too slow (2.0313s.) Setting batch_size=1.\n",
      "[Parallel(n_jobs=12)]: Done 12143 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.1845s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=12)]: Done 12254 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=12)]: Done 12408 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=12)]: Done 12566 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=12)]: Done 12724 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=12)]: Done 12886 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.1997s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=12)]: Done 13054 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=12)]: Done 13384 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=12)]: Done 13716 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=12)]: Done 14056 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=12)]: Done 14175 out of 14175 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here load a matrix with shape of 3,25,3,63,num_High_fre,num_Low_fre,num_time_window=6 cotaining PAC values\n",
    "Num_High_fre = gamma[1] - gamma[0] + 1\n",
    "Num_Low_fre = beta[1] - beta[0] + 1\n",
    "PAC_distribution_time_window = mvl_time\n",
    "\n",
    "\n",
    "# here you can set the dimention of Kernel filter and save path for\n",
    "Convolution_window_h = 4\n",
    "Convolution_window_l = 4\n",
    "Num_High_feature = Num_High_fre // Convolution_window_h\n",
    "Num_Low_feature = Num_Low_fre // Convolution_window_l\n",
    "Save_path = os.path.join(ds_path, 'PAC_time_conv_{suffix}.mmap')\n",
    "PAC_feature_time_window = np.memmap(Save_path, dtype=float, shape=(\n",
    "    3, 25, 3, 63, Num_High_feature, Num_Low_feature, 6), mode='w+')\n",
    "\n",
    "# # Test\n",
    "# PAC_feature_time_window[1,1,1,1,:,:,1] = Get_global_local_feature.conv_PAC_dist(PAC_distribution_time_window[1,1,1,1,:,:,1],\n",
    "#                                          h=Convolution_window_h,w=Convolution_window_l,down_sample_factor_w=Convolution_window_l,down_sample_factor_h=Convolution_window_h)\n",
    "\n",
    "\n",
    "PAC_feature_time_window = np.memmap('../data/Test.mmap', dtype=float, shape=(\n",
    "    3, 25, 3, 63, Num_High_feature, Num_Low_feature, 6), mode='w+')\n",
    "window_size = 200  # in ms\n",
    "down_sample_factor = 1\n",
    "window_sample = int(window_size*(0.5/down_sample_factor))\n",
    "frequency_h_n = 76\n",
    "frequency_l_n = 23\n",
    "start_high_frequency = 25\n",
    "end_high_frequency = 100\n",
    "start_low_frequency = 8\n",
    "end_low_frequency = 30\n",
    "frequency_range_high = np.linspace(\n",
    "    start_high_frequency, end_high_frequency, frequency_h_n).astype(np.int64)\n",
    "frequency_range_low = np.linspace(\n",
    "    start_low_frequency, end_low_frequency, frequency_l_n).astype(np.int64)\n",
    "\n",
    "# attention !!!! attention !!!! attention !!!!\n",
    "# If you run this cell for the second time or more, after possible crash in middle of the first run, you have to change the\n",
    "# \"load mode\" of np.mmap from 'w+' to 'r+', ohterwise, all the saved calculation, unitll the middle of the first run\n",
    "# are going to be removed by loading in 'w+' mode. After this change, there is no need to be worry of deleting previous\n",
    "# calculation you have done in next runs.\n",
    "\n",
    "\n",
    "PAC_feature_sub_on_electrodes = np.memmap('../data/Test.mmap', dtype=float, shape=(\n",
    "    3, 25, 3, 63, frequency_h_n, frequency_l_n, 6), mode='r+')\n",
    "# PAC_feature_sub_on_electrodes = np.zeros([3, 25, 3, 63,frequency_h_n ,frequency_l_n,6])\n",
    "print(PAC_feature_sub_on_electrodes.flags)\n",
    "\n",
    "\n",
    "def wrapper(i, sub, j, ch):\n",
    "    for t in np.arange(6):\n",
    "        PAC_feature_time_window[i, sub, j, ch, :, :, t] = \\\n",
    "            Get_global_local_feature.conv_PAC_dist(PAC_distribution_time_window[i, sub, j, ch, :, :, t],\n",
    "                                                   h=Convolution_window_h, w=Convolution_window_l,\n",
    "                                                   down_sample_factor_w=Convolution_window_l,\n",
    "                                                   down_sample_factor_h=Convolution_window_h)\n",
    "\n",
    "\n",
    "Parallel(n_jobs=12, verbose=10)(delayed(wrapper)(i, sub, j, ch) \\\n",
    "                                for i, sub, j, ch in itertools.product(range(3), range(25), range(3), range(nbchan)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P value of feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = np.zeros((PAC_feature_sub_on_electrodes.shape[3], 3, 3, 16, 4))\n",
    "# for ch in range(PAC_feature_sub_on_electrodes.shape[3]):\n",
    "for ch in list_index:\n",
    "    #     print(ch)\n",
    "    for grp, j in itertools.product(zip(itertools.combinations(range(3), 2), range(3)), range(3)):\n",
    "        a, b = grp[0]\n",
    "        i = grp[1]\n",
    "        test_data1, test_data2 = PAC_feature_sub_on_electrodes[a, :, j, ch].copy(\n",
    "        ), PAC_feature_sub_on_electrodes[b, :, j, ch].copy()\n",
    "        # for sub in range(PAC_feature_sub_on_electrodes.shape[1]):\n",
    "        #     test_data1[:, sub] = sig.convolve2d(test_data1[:, sub], filt_wind, boundary='symm', mode='same')\n",
    "        #     test_data2[:, sub] = sig.convolve2d(test_data2[:, sub], filt_wind, boundary='symm', mode='same')\n",
    "\n",
    "        t, p = scipy.stats.ttest_ind(test_data1, test_data2, equal_var=False)\n",
    "        p[p > 0.05] = 1\n",
    "        pv[ch, i, j] = (-np.log(p))\n",
    "\n",
    "i = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x axis ticks formater\n",
    "def format_func_x(value, tick_number):\n",
    "    # find number of multiples of pi/2\n",
    "    if tick_number % 2 == 1:\n",
    "        return f'({1+4*(value)},{1+4*(value+1)-1})'\n",
    "# y axis ticks formater\n",
    "\n",
    "\n",
    "def format_func_y(value, tick_number):\n",
    "    # find number of multiples of pi/2\n",
    "    if tick_number % 2 == 1:\n",
    "        return f'({10+4*(value)},{10+4*(value+1)-1})'\n",
    "\n",
    "\n",
    "gamma = [1, 16]\n",
    "beta = [1, 4]\n",
    "\n",
    "\n",
    "def plot_pac(pac, high_freq=gamma, low_freq=beta, ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(7, 15))\n",
    "        ax = fig.subplots()\n",
    "\n",
    "    im = ax.imshow((pac), origin='lower', interpolation='nearest',  # 'nearest',\n",
    "                   #                    aspect='auto', )\n",
    "                   aspect=np.diff(low_freq)/np.diff(high_freq), cmap='RdBu_r', **kwargs)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "    return im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_to_save_fig = 'D:\\Mastersharif\\MasterProject\\Working_with_kiani\\\\articles\\Result_on_articles\\Cbar_Corrected'\n",
    "temp = pv.sum(axis=0)\n",
    "vmax = temp.max()\n",
    "vmin = temp.min()\n",
    "max_1 = 0\n",
    "fig, axs = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(15, 15))\n",
    "#     vmin, vmax = get_percent(np.log(mvl_2ds[:, :, :, ch].mean(axis=1)), 0.83)\n",
    "for i, j in itertools.product(range(3), range(3)):\n",
    "    im = plot_pac(temp[i, j], ax=axs[i, j], vmin=vmin, vmax=vmax)\n",
    "#                       vmin=vmin vmax=vmax)\n",
    "\n",
    "    axs[i, j].xaxis.set_visible(False)\n",
    "    axs[i, j].yaxis.set_visible(False)\n",
    "    axs[i, j].xaxis.set_major_formatter(plt.FuncFormatter(format_func_x))\n",
    "    axs[i, j].yaxis.set_major_formatter(plt.FuncFormatter(format_func_y))\n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.02, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "# cbar = fig.colorbar(ims[i][j], ax=axs.ra//vel().tolist(), shrink=0.95)\n",
    "\n",
    "for grp in zip(itertools.combinations(range(3), 2), range(3)):\n",
    "    a, b = grp[0]\n",
    "    i = grp[1]\n",
    "\n",
    "    axs[i, 0].set_ylabel(f'{groups[a]} vs {groups[b]}')\n",
    "    axs[0, i].set_title(event_types[i])\n",
    "\n",
    "    axs[-1, i].xaxis.set_visible(True)\n",
    "    axs[i, 0].yaxis.set_visible(True)\n",
    "\n",
    "fname = f'neq_var_p_value_dist_feature_h_feature_l_window_dimetion_4_RdBu_r_Selected_Chs_corrected_cbar'\n",
    "file_path = os.path.join(Path_to_save_fig, fname)\n",
    "plt.savefig(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[3, 2, 5], [8, 1, 2], [6, 6, 7], [3, 5, 1]])\n",
    "index = np.linspace(a.shape[0]-1, 0, a.shape[0]).astype(np.int32)\n",
    "a[index, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How imshow plot work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "A = np.array([[3, 2, 5], [8, 1, 2], [6, 6, 7], [3, 5, 1]])  # The array to plot\n",
    "\n",
    "im = plt.imshow(A, origin=\"lower\", interpolation=\"nearest\", cmap=plt.cm.gray_r)\n",
    "plt.colorbar(im)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot topography of important feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_to_save_fig = 'D:\\Mastersharif\\MasterProject\\Working_with_kiani\\\\articles\\Result_on_articles\\\\topography'\n",
    "\n",
    "feature_h = 0\n",
    "feature_l = 1\n",
    "with open('..\\data\\channels_name.pkl', 'rb') as f:\n",
    "    channels_name = pickle.load(f)\n",
    "PAC_feature_sub_on_electrodes_gm = np.mean(\n",
    "    PAC_feature_sub_on_electrodes[:, :, :, :, feature_h, feature_l], axis=1)\n",
    "vmin = np.min(PAC_feature_sub_on_electrodes_gm)\n",
    "vmax = np.max(PAC_feature_sub_on_electrodes_gm)\n",
    "fig, axs = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(15, 15))\n",
    "\n",
    "mne_info = mne.create_info(ch_names=channels_name, sfreq=500., ch_types='eeg')\n",
    "\n",
    "montage = mne.channels.read_custom_montage('Standard-10-20-Cap81.locs')\n",
    "\n",
    "for i, j in itertools.product(range(3), range(3)):\n",
    "    #     data = mvls[i, :, j, :].mean(axis=0).reshape((-1, 1))\n",
    "    temp = np.expand_dims(PAC_feature_sub_on_electrodes_gm[i, j, :], axis=1)\n",
    "    PAC_grand_mean = mne.EvokedArray(temp, mne_info)\n",
    "    PAC_grand_mean.set_montage(montage)\n",
    "    im, cm = mne.viz.plot_topomap(PAC_grand_mean.data[:, 0],\n",
    "                                  PAC_grand_mean.info, axes=axs[i,\n",
    "                                                                j], show=False,\n",
    "                                  names=channels_name, show_names=True,\n",
    "                                  vmin=vmin, vmax=vmax, res=200, cmap='RdBu_r', fontsize='medium')\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i, 0].set_ylabel(groups[i])\n",
    "    axs[0, i].set_title(event_types[i])\n",
    "    axs[-1, i].xaxis.set_visible(True)\n",
    "    axs[i, 0].yaxis.set_visible(True)\n",
    "\n",
    "\n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.02, 0.7])\n",
    "clb = fig.colorbar(im, cax=cbar_ax)\n",
    "fname = f'feature_h_{feature_h}_feature_l_{feature_l}_RdBu_r'\n",
    "file_path = os.path.join(Path_to_save_fig, fname)\n",
    "plt.savefig(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P values on channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_h = 0\n",
    "feature_l = 1\n",
    "fig, axs = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(15, 15))\n",
    "# vmin, vmax = mvls.mean(axis=1).min(), mvls.mean(axis=1).max() #get_percent(mvls.mean(axis=1), 0.95)\n",
    "for grp, j in itertools.product(zip(itertools.combinations(range(3), 2), range(3)), range(3)):\n",
    "    a, b = grp[0]\n",
    "    i = grp[1]\n",
    "    data_a = PAC_feature_sub_on_electrodes[a, :, j, :, feature_h, feature_l]\n",
    "    data_b = PAC_feature_sub_on_electrodes[b, :, j, :, feature_h, feature_l]\n",
    "    t, p = scipy.stats.ttest_ind(data_a, data_b)\n",
    "    p[p > 0.05] = 1\n",
    "    p = p.reshape((-1, 1))\n",
    "    mvl_evoked = mne.EvokedArray(-np.log(p),  # - -np.log(p).mean(),\n",
    "                                 mne_info)\n",
    "    mvl_evoked.set_montage(montage)\n",
    "    im, cm = mne.viz.plot_topomap(mvl_evoked.data[:, 0],\n",
    "                                  mvl_evoked.info, axes=axs[i, j],\n",
    "                                  show=False,\n",
    "                                  names=channels_name, show_names=True, cmap='RdBu_r')\n",
    "    # , vmin=vmin- mvls.mean(), vmax=vmax- mvls.mean())\n",
    "\n",
    "for grp in zip(itertools.combinations(range(3), 2), range(3)):\n",
    "    a, b = grp[0]\n",
    "    i = grp[1]\n",
    "\n",
    "    axs[i, 0].set_ylabel(f'{groups[a]} vs {groups[b]}')\n",
    "    axs[0, i].set_title(event_types[i])\n",
    "\n",
    "    axs[-1, i].xaxis.set_visible(True)\n",
    "    axs[i, 0].yaxis.set_visible(True)\n",
    "\n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.02, 0.7])\n",
    "clb = fig.colorbar(im, cax=cbar_ax)\n",
    "fname = f'p_value_feature_h_{feature_h}_feature_l_{feature_l}_RdBu_r'\n",
    "file_path = os.path.join(Path_to_save_fig, fname)\n",
    "plt.savefig(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "03a4708f0c4954715ae9cf7024e72e3792c12398c409e26581e2d99117072eb2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
